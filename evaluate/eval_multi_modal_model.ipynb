{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ViBench"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys \n",
    "sys.path.append('../')\n",
    "\n",
    "import torch\n",
    "\n",
    "from models import build_model\n",
    "from models import PretrainModel_Phase\n",
    "from utils.base import seed_everything\n",
    "\n",
    "seed_everything(624)\n",
    "device = 'cpu'\n",
    "\n",
    "model = build_model('vib2mol_phase', spectral_channel=2).to(device)\n",
    "ckpt = torch.load('../checkpoints/mols/raman-ir-kekule_smiles/vib2mol_phase.pth', \n",
    "                  map_location=device, weights_only=True)\n",
    "\n",
    "ckpt = {k.replace('module.', ''): v for k, v in ckpt.items()}\n",
    "model.load_state_dict(ckpt, strict=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate contrastive retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/pytorch/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "100%|██████████| 588/588 [08:35<00:00,  1.14it/s]\n"
     ]
    }
   ],
   "source": [
    "from utils.dataloader import Dataloader\n",
    "from utils.collators import BaseCollator\n",
    "from utils.base import BaseEngine\n",
    "\n",
    "\n",
    "dataloader = Dataloader(lmdb_path='mols', \n",
    "                            data_dir='../datasets/vibench', \n",
    "                            target_keys=['raman', 'ir', 'kekule_smiles'], \n",
    "                            collate_fn=BaseCollator(spectral_types=['ir', 'raman'], tokenizer_path='../models/MolTokenizer'), \n",
    "                            device=device)\n",
    "\n",
    "test_loader = dataloader.generate_dataloader(mode='test', batch_size=64)\n",
    "\n",
    "engine = BaseEngine(test_loader=test_loader, model=model, device=device, device_rank=0)\n",
    "out = engine.infer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "recall@1:0.78829\n",
      "recall@3:0.95917\n",
      "recall@5:0.98126\n",
      "recall@10:0.99377\n",
      "recall@100:0.99952\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def calculate_similarity_matrix(embedding_query, embedding_key):\n",
    "    embedding_query = F.normalize(embedding_query, p=2, dim=1)\n",
    "    embedding_key = F.normalize(embedding_key, p=2, dim=1)\n",
    "\n",
    "    similarity_matrix = torch.matmul(embedding_query, embedding_key.t())\n",
    "    return similarity_matrix\n",
    "\n",
    "def compute_recall(similarity_matrix, k, verbose=False):\n",
    "    num_queries = similarity_matrix.size(0)\n",
    "    _, topk_indices = similarity_matrix.topk(k, dim=1, largest=True, sorted=True)\n",
    "    correct = 0\n",
    "    for i in range(num_queries):\n",
    "        if i in topk_indices[i]:\n",
    "            correct += 1\n",
    "    recall_at_k = correct / num_queries\n",
    "    \n",
    "    if verbose:\n",
    "        print(f'recall@{k}:{recall_at_k:.5f}')\n",
    "    else:\n",
    "        return recall_at_k\n",
    "\n",
    "similarity_matrix = calculate_similarity_matrix(out['spectral_proj_output'], out['molecular_proj_output'])\n",
    "compute_recall(similarity_matrix, k=1, verbose=True)\n",
    "compute_recall(similarity_matrix, k=3, verbose=True)\n",
    "compute_recall(similarity_matrix, k=5, verbose=True)\n",
    "compute_recall(similarity_matrix, k=10, verbose=True)\n",
    "compute_recall(similarity_matrix, k=100, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 spectrum-guided casual decoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lmdb\n",
    "# Open LMDB \n",
    "db = lmdb.open('../datasets/vibench/qm9/qm9_test.lmdb', subdir=False, lock=False, map_size=int(1e11))\n",
    "\n",
    "# Open a transaction and perform a read operation\n",
    "with db.begin() as txn:\n",
    "    test_data = list(txn.cursor())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 26687/26687 [00:00<00:00, 42281.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_len:34\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "import multiprocessing as mp \n",
    "from tqdm import tqdm \n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('../models/MolTokenizer')\n",
    "test_df =  pd.DataFrame([pickle.loads(item[1]) for item in tqdm(test_data)])\n",
    "length = [len(item) for item in test_df['kekule_smiles'].to_list()]\n",
    "max_len = max(length)+2\n",
    "print(f'max_len:{max_len}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 greedy generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 209/209 [01:39<00:00,  2.09it/s]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "class TestDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, raman, ir):\n",
    "        self.raman = raman\n",
    "        self.ir = ir\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.raman)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.raman[idx], self.ir[idx]\n",
    "\n",
    "class TestCollator:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    def __call__(self, batch):\n",
    "        tmp_raman, tmp_ir = zip(*batch)\n",
    "        tmp_raman = torch.as_tensor(np.array(tmp_raman), dtype=torch.float32).unsqueeze(1).to(device)\n",
    "        tmp_ir = torch.as_tensor(np.array(tmp_ir), dtype=torch.float32).unsqueeze(1).to(device)\n",
    "        return {'raman':tmp_raman, 'ir':tmp_ir}\n",
    "        # return {'ir':tmp_ir}\n",
    "\n",
    "all_pred_smiles = []\n",
    "# test_dataset = TestDataset(test_df['ir'].to_list(), test_df['ir'].to_list())\n",
    "test_dataset = TestDataset(test_df['raman'].to_list(), test_df['ir'].to_list())\n",
    "\n",
    "test_collator = TestCollator()\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=128, collate_fn=test_collator)\n",
    "test_bar = tqdm(test_loader)\n",
    "\n",
    "model.eval()\n",
    "for batch in test_bar:\n",
    "    with torch.no_grad():\n",
    "        pred_smiles_ids = model.infer_lm(batch, max_len=max_len)['pred_ids']\n",
    "    pred_smiles = tokenizer.batch_decode(pred_smiles_ids)\n",
    "    pred_smiles = [item.split('</s>')[0].replace('<s>', '') for item in pred_smiles]\n",
    "    all_pred_smiles.extend(pred_smiles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rdkit import Chem\n",
    "from rdkit import RDLogger\n",
    "RDLogger.DisableLog('rdApp.*')\n",
    "\n",
    "res_smiles = []\n",
    "for item in all_pred_smiles:\n",
    "    tmp_mol = Chem.MolFromSmiles(item)\n",
    "    if tmp_mol is not None:\n",
    "        tmp_smiles = Chem.MolToSmiles(tmp_mol, isomericSmiles=False, kekuleSmiles=True, canonical=True)\n",
    "    else:\n",
    "        tmp_smiles = '*'\n",
    "    res_smiles.append(tmp_smiles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rdkit import Chem\n",
    "from tqdm import trange\n",
    "\n",
    "def check_mols(pred_smiles, tgt_smiles):\n",
    "    pred_mol = Chem.MolFromSmiles(pred_smiles)\n",
    "    tgt_mol = Chem.MolFromSmiles(tgt_smiles)\n",
    "    if pred_mol is not None and tgt_mol is not None:\n",
    "        if Chem.MolToInchiKey(pred_mol) == Chem.MolToInchiKey(tgt_mol):\n",
    "            return 1\n",
    "    return 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 26687/26687 [00:25<00:00, 1049.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6651927904972459\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pred</th>\n",
       "      <th>tgt</th>\n",
       "      <th>filename</th>\n",
       "      <th>correct</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>COC1C2CC1C(=N)O2</td>\n",
       "      <td>COCC12CC1OC2=N</td>\n",
       "      <td>dsgdb9nsd_119549</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>O=C1C2CC1(CO)CO2</td>\n",
       "      <td>O=C1C2COC1(CO)C2</td>\n",
       "      <td>dsgdb9nsd_106611</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>OCC1C(O)C1O</td>\n",
       "      <td>OCC1C(O)C1O</td>\n",
       "      <td>dsgdb9nsd_003107</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CC1(O)CC2OC(=N)C21</td>\n",
       "      <td>CC1C(=N)OC2CC21O</td>\n",
       "      <td>dsgdb9nsd_075828</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>CC1OC(C=O)C1(C)C</td>\n",
       "      <td>CC1OC(C)C1(C)C=O</td>\n",
       "      <td>dsgdb9nsd_086297</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 pred               tgt          filename  correct\n",
       "0    COC1C2CC1C(=N)O2    COCC12CC1OC2=N  dsgdb9nsd_119549        0\n",
       "1    O=C1C2CC1(CO)CO2  O=C1C2COC1(CO)C2  dsgdb9nsd_106611        0\n",
       "2         OCC1C(O)C1O       OCC1C(O)C1O  dsgdb9nsd_003107        1\n",
       "3  CC1(O)CC2OC(=N)C21  CC1C(=N)OC2CC21O  dsgdb9nsd_075828        0\n",
       "4    CC1OC(C=O)C1(C)C  CC1OC(C)C1(C)C=O  dsgdb9nsd_086297        0"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame({'pred':res_smiles, \n",
    "                   'tgt':test_df['kekule_smiles'].to_list(), \n",
    "                   'filename':test_df['filename'].to_list(), \n",
    "                   'correct':[check_mols(res_smiles[i], test_df['kekule_smiles'].to_list()[i]) for i in trange(len(res_smiles))]})\n",
    "print(df.correct.mean())\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 beam search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 209/209 [10:43<00:00,  3.08s/it]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "class TestDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, raman, ir):\n",
    "        self.raman = raman\n",
    "        self.ir = ir\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.raman)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.raman[idx], self.ir[idx]\n",
    "\n",
    "class TestCollator:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    def __call__(self, batch):\n",
    "        tmp_raman, tmp_ir = zip(*batch)\n",
    "        tmp_raman = torch.as_tensor(np.array(tmp_raman), dtype=torch.float32).unsqueeze(1).to(device)\n",
    "        tmp_ir = torch.as_tensor(np.array(tmp_ir), dtype=torch.float32).unsqueeze(1).to(device)\n",
    "        return {'raman':tmp_raman, 'ir':tmp_ir}\n",
    "        # return {'ir':tmp_ir}\n",
    "\n",
    "beam_size = 10\n",
    "\n",
    "all_pred_smiles = []\n",
    "test_dataset = TestDataset(test_df['raman'].to_list(), test_df['ir'].to_list())\n",
    "test_collator = TestCollator()\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=128, collate_fn=test_collator)\n",
    "test_bar = tqdm(test_loader)\n",
    "\n",
    "model.eval()\n",
    "for batch in test_bar:\n",
    "    with torch.no_grad():\n",
    "        pred_smiles_ids_list = model.beam_infer_lm(batch, max_len=max_len, beam_size=beam_size, temperature=3.5)['pred_ids']\n",
    "    for pred_smiles_ids in pred_smiles_ids_list:\n",
    "        pred_smiles = tokenizer.batch_decode(pred_smiles_ids)\n",
    "        pred_smiles = [item.split('</s>')[0].replace('<s>', '') for item in pred_smiles]\n",
    "        all_pred_smiles.append(pred_smiles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### rank by beam score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rdkit import Chem\n",
    "from tqdm import trange\n",
    "from rdkit import RDLogger\n",
    "RDLogger.DisableLog('rdApp.*')\n",
    "\n",
    "def check_beam_mols(pred_smiles_list, tgt_smiles):\n",
    "    pred_mol_list = []\n",
    "    for item in pred_smiles_list:\n",
    "        mol = Chem.MolFromSmiles(item)\n",
    "        if mol is not None:\n",
    "            try:\n",
    "                inchi_key = Chem.MolToInchiKey(mol)\n",
    "                pred_mol_list.append(inchi_key)\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing SMILES {item}: {e}\")\n",
    "                pred_mol_list.append('')\n",
    "        else:\n",
    "            pred_mol_list.append('')\n",
    "    tgt_mol = Chem.MolToInchiKey(Chem.MolFromSmiles(tgt_smiles))\n",
    "    if tgt_mol in pred_mol_list:\n",
    "        return 1\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "top-1:\t\t0.69378\n",
      "top-3:\t\t0.79271\n",
      "top-5:\t\t0.79616\n",
      "top-10:\t\t0.79634\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "df = pd.DataFrame({'tgt_smiles':test_df['kekule_smiles'].to_list(), 'pred_smiles':[list(dict.fromkeys(item)) for item in all_pred_smiles]})\n",
    "df['top_1'] = df.apply(lambda row: check_beam_mols(row['pred_smiles'][:1], row['tgt_smiles']), axis=1)\n",
    "df['top_3'] = df.apply(lambda row: check_beam_mols(row['pred_smiles'][:3], row['tgt_smiles']), axis=1)\n",
    "df['top_5'] = df.apply(lambda row: check_beam_mols(row['pred_smiles'][:5], row['tgt_smiles']), axis=1)\n",
    "df['top_10'] = df.apply(lambda row: check_beam_mols(row['pred_smiles'][:10], row['tgt_smiles']), axis=1)\n",
    "\n",
    "print(f'top-1:\\t\\t{df.top_1.mean():.5f}\\ntop-3:\\t\\t{df.top_3.mean():.5f}\\ntop-5:\\t\\t{df.top_5.mean():.5f}\\ntop-10:\\t\\t{df.top_10.mean():.5f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lmdb\n",
    "# Open LMDB \n",
    "db = lmdb.open('../datasets/vibench/mols/mols_test.lmdb', subdir=False, lock=False, map_size=int(1e11))\n",
    "\n",
    "# Open a transaction and perform a read operation\n",
    "with db.begin() as txn:\n",
    "    test_data = list(txn.cursor())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 37570/37570 [00:00<00:00, 49084.88it/s]\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "import multiprocessing as mp \n",
    "from tqdm import tqdm \n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('../models/MolTokenizer')\n",
    "test_df =  pd.DataFrame([pickle.loads(item[1]) for item in tqdm(test_data)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mask_manually(input_ids, masked_indices):\n",
    "    mask = torch.zeros_like(input_ids).bool()\n",
    "    mask[:, torch.tensor(masked_indices)+1] = True\n",
    "    masked_ids = input_ids.clone()           \n",
    "    mask[masked_ids == 1] = False # tokenizer.pad_token_id = 1\n",
    "    masked_ids[mask] = 4 # tokenizer.mask_token_id = 4      \n",
    "    return masked_ids, mask\n",
    "\n",
    "def generate_mlmmask(input_ids, mask_prob=0.15):\n",
    "    masked_ids = input_ids.clone()\n",
    "    probability_matrix = torch.full(masked_ids.shape, mask_prob)\n",
    "    masked_indices = torch.bernoulli(probability_matrix).bool()\n",
    "    masked_indices[masked_ids == 1] = False # tokenizer.pad_token_id = 1\n",
    "    masked_ids[masked_indices] = 4 # tokenizer.mask_token_id = 4\n",
    "    \n",
    "    return masked_ids, masked_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C(=O)O NC1(<mask><mask><mask><mask><mask><mask>)CC1C1=CC=CC=C1\n",
      "=O NC1(C(<mask><mask>)O)CC1C1=CC=CC=C1\n"
     ]
    }
   ],
   "source": [
    "def extract_branches(s):\n",
    "    result = []\n",
    "    stack = []\n",
    "    start = -1\n",
    "    \n",
    "    for i, char in enumerate(s):\n",
    "        if char == '(':\n",
    "            if not stack:\n",
    "                start = i\n",
    "            stack.append(i)\n",
    "        elif char == ')':\n",
    "            if stack:\n",
    "                stack.pop()\n",
    "                if not stack:\n",
    "                    result.append(s[start+1:i])\n",
    "                    extract_nested = extract_branches(s[start+1:i])\n",
    "                    result.extend(extract_nested)\n",
    "    \n",
    "    return result\n",
    "\n",
    "test_string = \"NC1(C(=O)O)CC1C1=CC=CC=C1\"\n",
    "branches = extract_branches(test_string)\n",
    "for branch in branches:\n",
    "    masked_input = test_string.replace(branch, '<mask>'*len(branch))\n",
    "    print(branch, masked_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 evaluate molecular accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 37570/37570 [00:04<00:00, 8544.30it/s]\n"
     ]
    }
   ],
   "source": [
    "raw_smiles = []\n",
    "masked_smiles = []\n",
    "masked_spectra = []\n",
    "branches_list = []\n",
    "masked_smi_dict = {}\n",
    "\n",
    "mask_prob = None\n",
    "for i in trange(len(test_df)):\n",
    "    smi = test_df.iloc[i]['kekule_smiles']\n",
    "    raman = test_df.iloc[i]['raman']\n",
    "    ir = test_df.iloc[i]['ir']\n",
    "    branches = extract_branches(smi)\n",
    "    spec = np.vstack([raman, ir])\n",
    "    for branch in branches:\n",
    "        if mask_prob is None or (len(smi) * mask_prob < len(branch) and len(branch) <= len(smi) * (mask_prob + 0.15)):\n",
    "            len_token = len(tokenizer(branch)['input_ids'])-2\n",
    "            masked_smi = smi.replace(f\"({branch})\", f\"({'<mask>'*len_token})\")\n",
    "            if masked_smi not in masked_smi_dict:\n",
    "                branches_list.append(branch)\n",
    "                masked_smi_dict[masked_smi] = 1\n",
    "                masked_smiles.append(masked_smi)\n",
    "                masked_spectra.append(spec)\n",
    "                raw_smiles.append(smi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 580/580 [01:02<00:00,  9.25it/s]\n"
     ]
    }
   ],
   "source": [
    "from copy import deepcopy\n",
    "import numpy as np \n",
    "\n",
    "all_pred_smiles = []\n",
    "correct, total = 0, 0\n",
    "\n",
    "class TestDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, smiles, spectra):\n",
    "        self.smiles = smiles\n",
    "        self.spectra = spectra\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.smiles)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.smiles[idx], self.spectra[idx]\n",
    "\n",
    "class TestCollator:\n",
    "    def __init__(self, tokenizer):\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def __call__(self, batch):\n",
    "        smiles, spectra = zip(*batch)\n",
    "        spectra = torch.as_tensor(np.array(spectra), dtype=torch.float32).to(device)\n",
    "        # spectra = torch.as_tensor(np.array(spectra), dtype=torch.float32).unsqueeze(1).to(device)\n",
    "        input_ids = self.tokenizer(list(smiles), return_tensors='pt', padding='max_length', max_length=256, truncation=True)\n",
    "        input_ids = {'input_ids':input_ids['input_ids'].to(device), 'attention_mask':input_ids['attention_mask'].to(device)}\n",
    "        return {'smiles': input_ids,  'spectra':spectra}\n",
    "\n",
    "test_dataset = TestDataset(masked_smiles, masked_spectra)\n",
    "test_collator = TestCollator(tokenizer)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=64, collate_fn=test_collator)\n",
    "test_bar = tqdm(test_loader)\n",
    "\n",
    "model.eval()\n",
    "for batch in test_bar:\n",
    "    with torch.no_grad():\n",
    "        pred_tokens_logits = model.infer_mlm(batch)\n",
    "    pred_tokens = torch.argmax(pred_tokens_logits, dim=-1)\n",
    "    \n",
    "    output = deepcopy(batch['smiles']['input_ids'])\n",
    "    mask = (batch['smiles']['input_ids'] == 4).cpu()\n",
    "    output[mask] = pred_tokens[mask]\n",
    "    \n",
    "    preds = tokenizer.batch_decode(output, skip_special_tokens=True)\n",
    "    all_pred_smiles.extend(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 37100/37100 [00:07<00:00, 5038.57it/s]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from tqdm import trange\n",
    "\n",
    "tgt_fgs = []\n",
    "pred_fgs = []\n",
    "tgt_total = {}\n",
    "tgt_correct = {}\n",
    "\n",
    "for i in trange(len(raw_smiles)):\n",
    "\n",
    "    masked_fg = np.array(tokenizer(masked_smiles[i])['input_ids'])\n",
    "    tgt_fg = np.array(tokenizer(raw_smiles[i])['input_ids'])\n",
    "    pred_fg = np.array(tokenizer(all_pred_smiles[i])['input_ids'])[:len(masked_fg)]\n",
    "\n",
    "    # 找出所有mask的索引\n",
    "    indices_of_mask = np.where(masked_fg == 4)[0]\n",
    "\n",
    "    # 找出所有连续mask的索引\n",
    "    consecutive_indices = []\n",
    "    current_list = []\n",
    "\n",
    "    for i in range(len(indices_of_mask)):\n",
    "        if i == 0 or indices_of_mask[i] == indices_of_mask[i - 1] + 1:\n",
    "            current_list.append(indices_of_mask[i])\n",
    "        else:\n",
    "            if current_list:\n",
    "                consecutive_indices.append(current_list)\n",
    "            current_list = [indices_of_mask[i]]\n",
    "    if current_list:\n",
    "        consecutive_indices.append(current_list)\n",
    "\n",
    "\n",
    "    for mask in consecutive_indices:\n",
    "        if len(mask) == 1: mask = mask[0]\n",
    "        tgt_fg_str = tokenizer.decode(tgt_fg[mask])\n",
    "        pred_fg_str = tokenizer.decode(pred_fg[mask])\n",
    "        tgt_fgs.append(tgt_fg_str)\n",
    "        pred_fgs.append(pred_fg_str)\n",
    "\n",
    "        if tgt_fg_str in tgt_total:\n",
    "            tgt_total[tgt_fg_str] += 1\n",
    "        else:\n",
    "            tgt_total[tgt_fg_str] = 1\n",
    "        if pred_fg_str == tgt_fg_str and pred_fg_str not in tgt_correct:\n",
    "            tgt_correct[tgt_fg_str] = 1\n",
    "        elif pred_fg_str == tgt_fg_str and pred_fg_str  in tgt_correct:\n",
    "            tgt_correct[tgt_fg_str] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unique: 1744 | total: 41334 | correct: 38793.0 | accuracy: 0.93852518507766\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df1 = pd.DataFrame({'fg':tgt_total.keys(), 'count':tgt_total.values()})\n",
    "df2 = pd.DataFrame({'fg':tgt_correct.keys(), 'correct':tgt_correct.values()})\n",
    "df = pd.merge(df1, df2, on='fg', how='left')\n",
    "df[df.isna()] = 0\n",
    "df['accuracy'] = df['correct'] / df['count']\n",
    "print(f\"unique: {len(df)} | total: {df['count'].sum()} | correct: {df['correct'].sum()} | accuracy: {df['correct'].sum() / df['count'].sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NIST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IR-only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys \n",
    "sys.path.append('../')\n",
    "\n",
    "import torch\n",
    "\n",
    "from models import build_model\n",
    "from models import PretrainModel_Phase\n",
    "from utils.base import seed_everything\n",
    "\n",
    "seed_everything(624)\n",
    "device = 'cuda:0'\n",
    "\n",
    "model = build_model('vib2mol_cl', spectral_channel=1).to(device)\n",
    "ckpt = torch.load('/inspire/hdd/ws-f4d69b29-e0a5-44e6-bd92-acf4de9990f0/public-project/luxinyu-240207020178/vib2mol/checkpoints/nist_ir/exp_ir-kekule_smiles/vib2mol_cl/2025-01-03_07:39/epoch835_recall37.pth', \n",
    "                  map_location=device, weights_only=True)\n",
    "\n",
    "ckpt = {k.replace('module.', ''): v for k, v in ckpt.items()}\n",
    "model.load_state_dict(ckpt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate contrastive retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.base import Dataloader\n",
    "from utils.pretrain import PretrainCollator, Engine\n",
    "\n",
    "dataloader = Dataloader(lmdb_path='nist_ir', \n",
    "                            root_dir='/inspire/hdd/ws-f4d69b29-e0a5-44e6-bd92-acf4de9990f0/public-project/luxinyu-240207020178/vib2mol/datasets/historical', \n",
    "                            target_keys=['exp_ir', 'kekule_smiles'], \n",
    "                            collate_fn=PretrainCollator(spectral_types=['exp_ir'], tokenizer_path='/inspire/hdd/ws-f4d69b29-e0a5-44e6-bd92-acf4de9990f0/public-project/luxinyu-240207020178/vib2mol/models/MolTokenizer'), \n",
    "                            device=device)\n",
    "\n",
    "test_loader = dataloader.generate_dataloader(mode='test', batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 41/41 [00:01<00:00, 20.64it/s]\n"
     ]
    }
   ],
   "source": [
    "engine = Engine(test_loader=test_loader, model=model, device=device, device_rank=0)\n",
    "out = engine.infer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "recall@1:0.31453\n",
      "recall@3:0.46561\n",
      "recall@5:0.52898\n",
      "recall@10:0.60935\n",
      "recall@100:0.85858\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def calculate_similarity_matrix(embedding_query, embedding_key):\n",
    "    embedding_query = F.normalize(embedding_query, p=2, dim=1)\n",
    "    embedding_key = F.normalize(embedding_key, p=2, dim=1)\n",
    "\n",
    "    similarity_matrix = torch.matmul(embedding_query, embedding_key.t())\n",
    "    return similarity_matrix\n",
    "\n",
    "def compute_recall(similarity_matrix, k, verbose=False):\n",
    "    num_queries = similarity_matrix.size(0)\n",
    "    _, topk_indices = similarity_matrix.topk(k, dim=1, largest=True, sorted=True)\n",
    "    correct = 0\n",
    "    for i in range(num_queries):\n",
    "        if i in topk_indices[i]:\n",
    "            correct += 1\n",
    "    recall_at_k = correct / num_queries\n",
    "    \n",
    "    if verbose:\n",
    "        print(f'recall@{k}:{recall_at_k:.5f}')\n",
    "    else:\n",
    "        return recall_at_k\n",
    "\n",
    "similarity_matrix = calculate_similarity_matrix(out['spectral_proj_output'], out['molecular_proj_output'])\n",
    "compute_recall(similarity_matrix, k=1, verbose=True)\n",
    "compute_recall(similarity_matrix, k=3, verbose=True)\n",
    "compute_recall(similarity_matrix, k=5, verbose=True)\n",
    "compute_recall(similarity_matrix, k=10, verbose=True)\n",
    "compute_recall(similarity_matrix, k=100, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lmdb\n",
    "# Open LMDB \n",
    "db = lmdb.open('../datasets/nist_ir/nist_ir_test.lmdb', subdir=False, lock=False, map_size=int(1e11))\n",
    "\n",
    "# Open a transaction and perform a read operation\n",
    "with db.begin() as txn:\n",
    "    test_data = list(txn.cursor())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2588/2588 [00:00<00:00, 13742.21it/s]\n",
      "100%|██████████| 2588/2588 [00:00<00:00, 19077.08it/s]\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import multiprocessing as mp \n",
    "from tqdm import tqdm \n",
    "\n",
    "def get_smiles(idx):\n",
    "    try:\n",
    "        smiles = pickle.loads(test_data[idx][1])['kekule_smiles']\n",
    "        return smiles\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "# def get_raman(idx):\n",
    "#     raman = pickle.loads(test_data[idx][1])['raman']\n",
    "#     return raman\n",
    "\n",
    "def get_ir(idx):\n",
    "    ir = pickle.loads(test_data[idx][1])['exp_ir']\n",
    "    return ir\n",
    "\n",
    "with mp.Pool(4) as pool:\n",
    "    smiles = list(tqdm(pool.imap(get_smiles, range(len(test_data))), total=len(test_data)))\n",
    "\n",
    "with mp.Pool(4) as pool:\n",
    "    ir = list(tqdm(pool.imap(get_ir, range(len(test_data))), total=len(test_data)))\n",
    "\n",
    "from transformers import RobertaTokenizer\n",
    "tokenizer = RobertaTokenizer.from_pretrained('../models/RoBERTa')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CC(C)(C)O 9\n"
     ]
    }
   ],
   "source": [
    "\n",
    "demo_idx = 1\n",
    "demo_smiles = smiles[demo_idx]\n",
    "demo_spectra = ir[demo_idx]\n",
    "demo_smiles_input = tokenizer(demo_smiles, return_tensors='pt', padding='max_length', max_length=256, truncation=True)\n",
    "\n",
    "print(demo_smiles, len(demo_smiles))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mask_manually(input_ids, masked_indices):\n",
    "    mask = torch.zeros_like(input_ids).bool()\n",
    "    mask[:, torch.tensor(masked_indices)+1] = True\n",
    "    masked_ids = input_ids.clone()           \n",
    "    mask[masked_ids == 1] = False # tokenizer.pad_token_id = 1\n",
    "    masked_ids[mask] = 4 # tokenizer.mask_token_id = 4      \n",
    "    return masked_ids, mask\n",
    "\n",
    "def generate_mlmmask(input_ids, mask_prob=0.15):\n",
    "    masked_ids = input_ids.clone()\n",
    "    probability_matrix = torch.full(masked_ids.shape, mask_prob)         \n",
    "    masked_indices = torch.bernoulli(probability_matrix).bool()                                  \n",
    "    masked_indices[masked_ids == 1] = False # tokenizer.pad_token_id = 1\n",
    "    masked_ids[masked_indices] = 4 # tokenizer.mask_token_id = 4\n",
    "    \n",
    "    return masked_ids, masked_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C(=O)O NC1(<mask><mask><mask><mask><mask><mask>)CC1C1=CC=CC=C1\n",
      "=O NC1(C(<mask><mask>)O)CC1C1=CC=CC=C1\n"
     ]
    }
   ],
   "source": [
    "def extract_branches(s):\n",
    "    result = []\n",
    "    stack = []\n",
    "    start = -1\n",
    "    \n",
    "    for i, char in enumerate(s):\n",
    "        if char == '(':\n",
    "            if not stack:\n",
    "                start = i\n",
    "            stack.append(i)\n",
    "        elif char == ')':\n",
    "            if stack:\n",
    "                stack.pop()\n",
    "                if not stack:\n",
    "                    result.append(s[start+1:i])\n",
    "                    extract_nested = extract_branches(s[start+1:i])\n",
    "                    result.extend(extract_nested)\n",
    "    \n",
    "    return result\n",
    "\n",
    "test_string = \"NC1(C(=O)O)CC1C1=CC=CC=C1\"\n",
    "branches = extract_branches(test_string)\n",
    "for branch in branches:\n",
    "    masked_input = test_string.replace(branch, '<mask>'*len(branch))\n",
    "    print(branch, masked_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### demon the reconstruction of masked tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target:  N#CC#N\n",
      "masked:  N#<mask>C#N\n",
      "pred:    N#CC#N\n"
     ]
    }
   ],
   "source": [
    "demo_idx = 53\n",
    "demo_smiles = smiles[demo_idx]\n",
    "demo_spectra = ir[demo_idx]\n",
    "demo_smiles_input = tokenizer(demo_smiles, return_tensors='pt', padding='max_length', max_length=256, truncation=True)\n",
    "\n",
    "# mask smiles manually\n",
    "masked_idx = [2]\n",
    "# masked_idx = [i for i in range(13, 24)]\n",
    "masked_smiles_tuple = mask_manually(demo_smiles_input['input_ids'], masked_idx)\n",
    "\n",
    "# or mask smiles randomly\n",
    "# masked_smiles_tuple = generate_mlmmask(demo_smiles_input['input_ids'], mask_prob=0.75)\n",
    "\n",
    "masked_smiles = tokenizer.decode(masked_smiles_tuple[0].flatten()).replace('<pad>', '').replace('<s>', '').replace('</s>', '')\n",
    "\n",
    "model.eval()\n",
    "masked_smiles_input = {'input_ids':masked_smiles_tuple[0].to(device), 'attention_mask':demo_smiles_input['attention_mask'].to(device)}\n",
    "pred_tokens_logits = model.infer({'smiles': masked_smiles_input, 'spectra': torch.tensor(demo_spectra, dtype=torch.float32).to(device).unsqueeze(0).unsqueeze(0)})\n",
    "\n",
    "pred_tokens = torch.argmax(pred_tokens_logits, dim=-1)\n",
    "pred_smiles = tokenizer.decode(pred_tokens.flatten())[3:len(demo_smiles)+3] # remove <s>\n",
    "\n",
    "print('target: ', tokenizer.decode(demo_smiles_input['input_ids'].flatten()).replace('<pad>', '').replace('<s>', '').replace('</s>', ''))\n",
    "print('masked: ', masked_smiles)\n",
    "print('pred:   ', pred_smiles) # remove <s>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### evaluate molecular accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2588/2588 [00:00<00:00, 7760.87it/s]\n"
     ]
    }
   ],
   "source": [
    "raw_smiles = []\n",
    "masked_smiles = []\n",
    "masked_spectra = []\n",
    "branches_list = []\n",
    "masked_smi_dict = {}\n",
    "\n",
    "mask_prob = None\n",
    "for smi, spec in tqdm(zip(smiles, ir), total=len(smiles)):\n",
    "    branches = extract_branches(smi)\n",
    "    for branch in branches:\n",
    "        if mask_prob is None or (len(smi) * mask_prob < len(branch) and len(branch) <= len(smi) * (mask_prob + 0.15)):\n",
    "            len_token = len(tokenizer(branch)['input_ids'])-2\n",
    "            masked_smi = smi.replace(f\"({branch})\", f\"({'<mask>'*len_token})\")\n",
    "            if masked_smi not in masked_smi_dict:\n",
    "                branches_list.append(branch)\n",
    "                masked_smi_dict[masked_smi] = 1\n",
    "                masked_smiles.append(masked_smi)\n",
    "                masked_spectra.append(spec)\n",
    "                raw_smiles.append(smi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3819, 3819)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(len(raw_smiles), len(branches_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 60/60 [00:16<00:00,  3.57it/s]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "all_pred_smiles = []\n",
    "correct, total = 0, 0\n",
    "\n",
    "class TestDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, smiles, spectra):\n",
    "        self.smiles = smiles\n",
    "        self.spectra = spectra\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.smiles)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.smiles[idx], self.spectra[idx]\n",
    "\n",
    "class TestCollator:\n",
    "    def __init__(self, tokenizer):\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def __call__(self, batch):\n",
    "        smiles, spectra = zip(*batch)\n",
    "        spectra = torch.as_tensor(np.array(spectra), dtype=torch.float32).unsqueeze(1).to(device)\n",
    "        input_ids = self.tokenizer(list(smiles), return_tensors='pt', padding='max_length', max_length=256, truncation=True)\n",
    "        input_ids = {'input_ids':input_ids['input_ids'].to(device), 'attention_mask':input_ids['attention_mask'].to(device)}\n",
    "        return {'smiles': input_ids,  'spectra':spectra}\n",
    "\n",
    "test_dataset = TestDataset(masked_smiles, masked_spectra)\n",
    "test_collator = TestCollator(tokenizer)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=64, collate_fn=test_collator)\n",
    "test_bar = tqdm(test_loader)\n",
    "\n",
    "model.eval()\n",
    "for batch in test_bar:\n",
    "    with torch.no_grad():\n",
    "        pred_tokens_logits = model.infer(batch)\n",
    "    pred_tokens = torch.argmax(pred_tokens_logits, dim=-1)\n",
    "\n",
    "    preds = tokenizer.batch_decode(pred_tokens, skip_special_tokens=True)\n",
    "    all_pred_smiles.extend(preds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3819/3819 [00:01<00:00, 2124.35it/s]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from tqdm import trange\n",
    "\n",
    "tgt_fgs = []\n",
    "pred_fgs = []\n",
    "tgt_total = {}\n",
    "tgt_correct = {}\n",
    "\n",
    "for i in trange(len(raw_smiles)):\n",
    "    \n",
    "    masked_fg = np.array(tokenizer(masked_smiles[i])['input_ids'])\n",
    "    tgt_fg = np.array(tokenizer(raw_smiles[i])['input_ids'])\n",
    "    pred_fg = np.array(tokenizer(all_pred_smiles[i])['input_ids'])[:len(masked_fg)]\n",
    "\n",
    "    # 找出所有4的索引\n",
    "    indices_of_mask = np.where(masked_fg == 4)[0]\n",
    "\n",
    "    # 找出所有连续的4的索引\n",
    "    consecutive_indices = []\n",
    "    current_list = []\n",
    "\n",
    "    for i in range(len(indices_of_mask)):\n",
    "        if i == 0 or indices_of_mask[i] == indices_of_mask[i - 1] + 1:\n",
    "            current_list.append(indices_of_mask[i])\n",
    "        else:\n",
    "            if current_list:\n",
    "                consecutive_indices.append(current_list)\n",
    "            current_list = [indices_of_mask[i]]\n",
    "    if current_list:\n",
    "        consecutive_indices.append(current_list)\n",
    "\n",
    "\n",
    "    for mask in consecutive_indices:\n",
    "        if len(mask) == 1: mask = mask[0]\n",
    "        tgt_fg_str = tokenizer.decode(tgt_fg[mask])\n",
    "        pred_fg_str = tokenizer.decode(pred_fg[mask])\n",
    "        tgt_fgs.append(tgt_fg_str)\n",
    "        pred_fgs.append(pred_fg_str)\n",
    "\n",
    "        if tgt_fg_str in tgt_total:\n",
    "            tgt_total[tgt_fg_str] += 1\n",
    "        else:\n",
    "            tgt_total[tgt_fg_str] = 1\n",
    "        if pred_fg_str == tgt_fg_str and pred_fg_str not in tgt_correct:\n",
    "            tgt_correct[tgt_fg_str] = 1\n",
    "        elif pred_fg_str == tgt_fg_str and pred_fg_str  in tgt_correct:\n",
    "            tgt_correct[tgt_fg_str] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unique: 506 | total: 5035 | correct: 3502.0 | accuracy: 0.6955312810327706\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df1 = pd.DataFrame({'fg':tgt_total.keys(), 'count':tgt_total.values()})\n",
    "df2 = pd.DataFrame({'fg':tgt_correct.keys(), 'correct':tgt_correct.values()})\n",
    "df = pd.merge(df1, df2, on='fg', how='left')\n",
    "df[df.isna()] = 0\n",
    "df['accuracy'] = df['correct'] / df['count']\n",
    "print(f\"unique: {len(df)} | total: {df['count'].sum()} | correct: {df['correct'].sum()} | accuracy: {df['correct'].sum() / df['count'].sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IR-Raman"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys \n",
    "sys.path.append('../')\n",
    "\n",
    "import torch\n",
    "\n",
    "from models import build_model\n",
    "from models import PretrainModel_Phase\n",
    "from utils.base import seed_everything\n",
    "\n",
    "seed_everything(624)\n",
    "device = 'cuda:0'\n",
    "\n",
    "model = build_model('vib2mol_phase', spectral_channel=2).to(device)\n",
    "ckpt = torch.load('/inspire/hdd/ws-f4d69b29-e0a5-44e6-bd92-acf4de9990f0/public-project/luxinyu-240207020178/vib2mol/checkpoints/mols_custom/raman-ir-kekule_smiles/vib2mol_phase/2025-01-01_04:18/epoch990_acc76.pth', \n",
    "                  map_location=device, weights_only=True)\n",
    "\n",
    "# model = build_model('vib2mol_cl_lm').to(device)\n",
    "# ckpt = torch.load('/inspire/hdd/ws-f4d69b29-e0a5-44e6-bd92-acf4de9990f0/public-project/luxinyu-240207020178/vib2mol/checkpoints/mols_custom/raman-kekule_smiles/vib2mol_cl_lm/2024-12-03_13:38/epoch952_recall74.pth', \n",
    "#                   map_location=device, weights_only=True)\n",
    "\n",
    "ckpt = {k.replace('module.', ''): v for k, v in ckpt.items()}\n",
    "model.load_state_dict(ckpt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 Evaluate contrastive retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.base import Dataloader\n",
    "from utils.pretrain import PretrainCollator, Engine\n",
    "\n",
    "dataloader = Dataloader(lmdb_path='mols', \n",
    "                            root_dir='/inspire/hdd/ws-f4d69b29-e0a5-44e6-bd92-acf4de9990f0/public-project/luxinyu-240207020178/vib2mol/datasets/vibbench', \n",
    "                            # target_keys=['exp_ir', 'kekule_smiles'], \n",
    "                            # collate_fn=PretrainCollator(spectral_types=['exp_ir'], tokenizer_path='/data/xinyulu/vib2mol/models/RoBERTa'), \n",
    "                            target_keys=['raman', 'ir', 'kekule_smiles'], \n",
    "                            collate_fn=PretrainCollator(spectral_types=['raman','ir'], tokenizer_path='/inspire/hdd/ws-f4d69b29-e0a5-44e6-bd92-acf4de9990f0/public-project/luxinyu-240207020178/vib2mol/models/MolTokenizer'), \n",
    "                            device=device)\n",
    "\n",
    "test_loader = dataloader.generate_dataloader(mode='test', batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 294/294 [00:19<00:00, 15.02it/s]\n"
     ]
    }
   ],
   "source": [
    "engine = Engine(test_loader=test_loader, model=model, device=device, device_rank=0)\n",
    "out = engine.infer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "recall@1:0.00005\n",
      "recall@3:0.00027\n",
      "recall@5:0.00032\n",
      "recall@10:0.00059\n",
      "recall@100:0.00511\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def calculate_similarity_matrix(embedding_query, embedding_key):\n",
    "    embedding_query = F.normalize(embedding_query, p=2, dim=1)\n",
    "    embedding_key = F.normalize(embedding_key, p=2, dim=1)\n",
    "\n",
    "    similarity_matrix = torch.matmul(embedding_query, embedding_key.t())\n",
    "    return similarity_matrix\n",
    "\n",
    "def compute_recall(similarity_matrix, k, verbose=False):\n",
    "    num_queries = similarity_matrix.size(0)\n",
    "    _, topk_indices = similarity_matrix.topk(k, dim=1, largest=True, sorted=True)\n",
    "    correct = 0\n",
    "    for i in range(num_queries):\n",
    "        if i in topk_indices[i]:\n",
    "            correct += 1\n",
    "    recall_at_k = correct / num_queries\n",
    "    \n",
    "    if verbose:\n",
    "        print(f'recall@{k}:{recall_at_k:.5f}')\n",
    "    else:\n",
    "        return recall_at_k\n",
    "\n",
    "similarity_matrix = calculate_similarity_matrix(out['spectral_proj_output'], out['molecular_proj_output'])\n",
    "compute_recall(similarity_matrix, k=1, verbose=True)\n",
    "compute_recall(similarity_matrix, k=3, verbose=True)\n",
    "compute_recall(similarity_matrix, k=5, verbose=True)\n",
    "compute_recall(similarity_matrix, k=10, verbose=True)\n",
    "compute_recall(similarity_matrix, k=100, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lmdb\n",
    "# Open LMDB \n",
    "db = lmdb.open('../datasets/nist_ir/nist_ir_test.lmdb', subdir=False, lock=False, map_size=int(1e11))\n",
    "\n",
    "# Open a transaction and perform a read operation\n",
    "with db.begin() as txn:\n",
    "    test_data = list(txn.cursor())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2588/2588 [00:00<00:00, 22656.11it/s]\n",
      "100%|██████████| 2588/2588 [00:00<00:00, 12259.45it/s]\n",
      "100%|██████████| 2588/2588 [00:00<00:00, 16320.57it/s]\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import multiprocessing as mp \n",
    "from tqdm import tqdm \n",
    "\n",
    "def get_smiles(idx):\n",
    "    try:\n",
    "        smiles = pickle.loads(test_data[idx][1])['kekule_smiles']\n",
    "        return smiles\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "def get_raman(idx):\n",
    "    raman = pickle.loads(test_data[idx][1])['raman']\n",
    "    return raman\n",
    "\n",
    "def get_ir(idx):\n",
    "    ir = pickle.loads(test_data[idx][1])['exp_ir']\n",
    "    return ir\n",
    "\n",
    "with mp.Pool(4) as pool:\n",
    "    smiles = list(tqdm(pool.imap(get_smiles, range(len(test_data))), total=len(test_data)))\n",
    "\n",
    "with mp.Pool(4) as pool:\n",
    "    ir = list(tqdm(pool.imap(get_ir, range(len(test_data))), total=len(test_data)))\n",
    "\n",
    "with mp.Pool(4) as pool:\n",
    "    raman = list(tqdm(pool.imap(get_raman, range(len(test_data))), total=len(test_data)))\n",
    "    \n",
    "from transformers import RobertaTokenizer\n",
    "tokenizer = RobertaTokenizer.from_pretrained('../models/RoBERTa')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mask_manually(input_ids, masked_indices):\n",
    "    mask = torch.zeros_like(input_ids).bool()\n",
    "    mask[:, torch.tensor(masked_indices)+1] = True\n",
    "    masked_ids = input_ids.clone()           \n",
    "    mask[masked_ids == 1] = False # tokenizer.pad_token_id = 1\n",
    "    masked_ids[mask] = 4 # tokenizer.mask_token_id = 4      \n",
    "    return masked_ids, mask\n",
    "\n",
    "def generate_mlmmask(input_ids, mask_prob=0.15):\n",
    "    masked_ids = input_ids.clone()\n",
    "    probability_matrix = torch.full(masked_ids.shape, mask_prob)         \n",
    "    masked_indices = torch.bernoulli(probability_matrix).bool()                                  \n",
    "    masked_indices[masked_ids == 1] = False # tokenizer.pad_token_id = 1\n",
    "    masked_ids[masked_indices] = 4 # tokenizer.mask_token_id = 4\n",
    "    \n",
    "    return masked_ids, masked_indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 evaluate molecular accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/2588 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2588/2588 [00:00<00:00, 7975.33it/s]\n"
     ]
    }
   ],
   "source": [
    "spectra = np.stack([raman, ir], axis=1)\n",
    "\n",
    "raw_smiles = []\n",
    "masked_smiles = []\n",
    "masked_spectra = []\n",
    "branches_list = []\n",
    "masked_smi_dict = {}\n",
    "\n",
    "mask_prob = None\n",
    "for smi, spec in tqdm(zip(smiles, spectra), total=len(smiles)):\n",
    "    branches = extract_branches(smi)\n",
    "    for branch in branches:\n",
    "        if mask_prob is None or (len(smi) * mask_prob < len(branch) and len(branch) <= len(smi) * (mask_prob + 0.15)):\n",
    "            len_token = len(tokenizer(branch)['input_ids'])-2\n",
    "            masked_smi = smi.replace(f\"({branch})\", f\"({'<mask>'*len_token})\")\n",
    "            if masked_smi not in masked_smi_dict:\n",
    "                branches_list.append(branch)\n",
    "                masked_smi_dict[masked_smi] = 1\n",
    "                masked_smiles.append(masked_smi)\n",
    "                masked_spectra.append(spec)\n",
    "                raw_smiles.append(smi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3819, 3819)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(len(raw_smiles), len(branches_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 60/60 [00:16<00:00,  3.55it/s]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "all_pred_smiles = []\n",
    "correct, total = 0, 0\n",
    "\n",
    "class TestDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, smiles, spectra):\n",
    "        self.smiles = smiles\n",
    "        self.spectra = spectra\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.smiles)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.smiles[idx], self.spectra[idx]\n",
    "\n",
    "class TestCollator:\n",
    "    def __init__(self, tokenizer):\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def __call__(self, batch):\n",
    "        smiles, spectra = zip(*batch)\n",
    "        spectra = torch.as_tensor(np.array(spectra), dtype=torch.float32).to(device)\n",
    "        input_ids = self.tokenizer(list(smiles), return_tensors='pt', padding='max_length', max_length=256, truncation=True)\n",
    "        input_ids = {'input_ids':input_ids['input_ids'].to(device), 'attention_mask':input_ids['attention_mask'].to(device)}\n",
    "        return {'smiles': input_ids,  'spectra':spectra}\n",
    "\n",
    "test_dataset = TestDataset(masked_smiles, masked_spectra)\n",
    "test_collator = TestCollator(tokenizer)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=64, collate_fn=test_collator)\n",
    "test_bar = tqdm(test_loader)\n",
    "\n",
    "model.eval()\n",
    "for batch in test_bar:\n",
    "    with torch.no_grad():\n",
    "        pred_tokens_logits = model.infer(batch)\n",
    "    pred_tokens = torch.argmax(pred_tokens_logits, dim=-1)\n",
    "\n",
    "    preds = tokenizer.batch_decode(pred_tokens, skip_special_tokens=True)\n",
    "    all_pred_smiles.extend(preds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3819/3819 [00:01<00:00, 2121.59it/s]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from tqdm import trange\n",
    "\n",
    "tgt_fgs = []\n",
    "pred_fgs = []\n",
    "tgt_total = {}\n",
    "tgt_correct = {}\n",
    "\n",
    "for i in trange(len(raw_smiles)):\n",
    "    \n",
    "    masked_fg = np.array(tokenizer(masked_smiles[i])['input_ids'])\n",
    "    tgt_fg = np.array(tokenizer(raw_smiles[i])['input_ids'])\n",
    "    pred_fg = np.array(tokenizer(all_pred_smiles[i])['input_ids'])[:len(masked_fg)]\n",
    "\n",
    "    indices_of_mask = np.where(masked_fg == 4)[0]\n",
    "\n",
    "    consecutive_indices = []\n",
    "    current_list = []\n",
    "\n",
    "    for i in range(len(indices_of_mask)):\n",
    "        if i == 0 or indices_of_mask[i] == indices_of_mask[i - 1] + 1:\n",
    "            current_list.append(indices_of_mask[i])\n",
    "        else:\n",
    "            if current_list:\n",
    "                consecutive_indices.append(current_list)\n",
    "            current_list = [indices_of_mask[i]]\n",
    "    if current_list:\n",
    "        consecutive_indices.append(current_list)\n",
    "\n",
    "\n",
    "    for mask in consecutive_indices:\n",
    "        if len(mask) == 1: mask = mask[0]\n",
    "        tgt_fg_str = tokenizer.decode(tgt_fg[mask])\n",
    "        pred_fg_str = tokenizer.decode(pred_fg[mask])\n",
    "        tgt_fgs.append(tgt_fg_str)\n",
    "        pred_fgs.append(pred_fg_str)\n",
    "\n",
    "        if tgt_fg_str in tgt_total:\n",
    "            tgt_total[tgt_fg_str] += 1\n",
    "        else:\n",
    "            tgt_total[tgt_fg_str] = 1\n",
    "        if pred_fg_str == tgt_fg_str and pred_fg_str not in tgt_correct:\n",
    "            tgt_correct[tgt_fg_str] = 1\n",
    "        elif pred_fg_str == tgt_fg_str and pred_fg_str  in tgt_correct:\n",
    "            tgt_correct[tgt_fg_str] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unique: 506 | total: 5035 | correct: 3750.0 | accuracy: 0.7447864945382324\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df1 = pd.DataFrame({'fg':tgt_total.keys(), 'count':tgt_total.values()})\n",
    "df2 = pd.DataFrame({'fg':tgt_correct.keys(), 'correct':tgt_correct.values()})\n",
    "df = pd.merge(df1, df2, on='fg', how='left')\n",
    "df[df.isna()] = 0\n",
    "df['accuracy'] = df['correct'] / df['count']\n",
    "print(f\"unique: {len(df)} | total: {df['count'].sum()} | correct: {df['correct'].sum()} | accuracy: {df['correct'].sum() / df['count'].sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
